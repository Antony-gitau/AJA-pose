{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ALICJYW_inncLY27qKcgvsCp7fwVfsVm",
      "authorship_tag": "ABX9TyPZ+I0799znI4azizUIlMvF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antony-gitau/2024_ICME_Challenge/blob/main/config_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV59ceuo3lHk",
        "outputId": "ffb85062-8394-40d1-a72f-11ea5f82f737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'VHR-BirdPose'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/38)\u001b[K\rremote: Counting objects:   5% (2/38)\u001b[K\rremote: Counting objects:   7% (3/38)\u001b[K\rremote: Counting objects:  10% (4/38)\u001b[K\rremote: Counting objects:  13% (5/38)\u001b[K\rremote: Counting objects:  15% (6/38)\u001b[K\rremote: Counting objects:  18% (7/38)\u001b[K\rremote: Counting objects:  21% (8/38)\u001b[K\rremote: Counting objects:  23% (9/38)\u001b[K\rremote: Counting objects:  26% (10/38)\u001b[K\rremote: Counting objects:  28% (11/38)\u001b[K\rremote: Counting objects:  31% (12/38)\u001b[K\rremote: Counting objects:  34% (13/38)\u001b[K\rremote: Counting objects:  36% (14/38)\u001b[K\rremote: Counting objects:  39% (15/38)\u001b[K\rremote: Counting objects:  42% (16/38)\u001b[K\rremote: Counting objects:  44% (17/38)\u001b[K\rremote: Counting objects:  47% (18/38)\u001b[K\rremote: Counting objects:  50% (19/38)\u001b[K\rremote: Counting objects:  52% (20/38)\u001b[K\rremote: Counting objects:  55% (21/38)\u001b[K\rremote: Counting objects:  57% (22/38)\u001b[K\rremote: Counting objects:  60% (23/38)\u001b[K\rremote: Counting objects:  63% (24/38)\u001b[K\rremote: Counting objects:  65% (25/38)\u001b[K\rremote: Counting objects:  68% (26/38)\u001b[K\rremote: Counting objects:  71% (27/38)\u001b[K\rremote: Counting objects:  73% (28/38)\u001b[K\rremote: Counting objects:  76% (29/38)\u001b[K\rremote: Counting objects:  78% (30/38)\u001b[K\rremote: Counting objects:  81% (31/38)\u001b[K\rremote: Counting objects:  84% (32/38)\u001b[K\rremote: Counting objects:  86% (33/38)\u001b[K\rremote: Counting objects:  89% (34/38)\u001b[K\rremote: Counting objects:  92% (35/38)\u001b[K\rremote: Counting objects:  94% (36/38)\u001b[K\rremote: Counting objects:  97% (37/38)\u001b[K\rremote: Counting objects: 100% (38/38)\u001b[K\rremote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 38 (delta 11), reused 35 (delta 11), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (38/38), 28.75 KiB | 2.40 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/LuoXishuang0712/VHR-BirdPose.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preparing the environment according to HRNET"
      ],
      "metadata": {
        "id": "QnRdaqYhVm7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axiyk9EjWSlD",
        "outputId": "e3ff5c88-c436-4677-c94e-8477ef4c7e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/leoxiaobin/deep-high-resolution-net.pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xiJF-WZWWHc",
        "outputId": "3d1b375b-51ed-4165-883e-873bec4ee7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-high-resolution-net.pytorch'...\n",
            "remote: Enumerating objects: 296, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 296 (delta 0), reused 2 (delta 0), pack-reused 293\u001b[K\n",
            "Receiving objects: 100% (296/296), 15.56 MiB | 21.21 MiB/s, done.\n",
            "Resolving deltas: 100% (137/137), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/deep-high-resolution-net.pytorch/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJdO5pLxXL5n",
        "outputId": "517f370b-3674-4c04-fba0-23edf5d7c8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: EasyDict==1.7 in /usr/local/lib/python3.10/dist-packages (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 1)) (1.7)\n",
            "Requirement already satisfied: opencv-python==4.8.0.76 in /usr/local/lib/python3.10/dist-packages (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 2)) (4.8.0.76)\n",
            "Collecting shapely==1.7.1 (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 3))\n",
            "  Downloading Shapely-1.7.1.tar.gz (383 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 4)) (3.0.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 5)) (1.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 7)) (6.0.1)\n",
            "Requirement already satisfied: json_tricks in /usr/local/lib/python3.10/dist-packages (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 8)) (3.17.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 9)) (0.19.3)\n",
            "Requirement already satisfied: yacs>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 10)) (0.1.8)\n",
            "Requirement already satisfied: tensorboardX==1.6 in /usr/local/lib/python3.10/dist-packages (from -r /content/deep-high-resolution-net.pytorch/requirements.txt (line 11)) (1.6)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python==4.8.0.76->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.6->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboardX==1.6->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 6)) (2023.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 9)) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 9)) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 9)) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 9)) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/deep-high-resolution-net.pytorch/requirements.txt (line 9)) (23.2)\n",
            "Building wheels for collected packages: shapely\n",
            "  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shapely: filename=Shapely-1.7.1-cp310-cp310-linux_x86_64.whl size=997441 sha256=f512e83ec17d506c42ef7244f82d0ad8c94d2c611c527c5e3531c6daca91ab99\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/fa/97/c85f587c35afcaf4a81c481741d36592518d1e50445572f0d4\n",
            "Successfully built shapely\n",
            "Installing collected packages: shapely\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: Shapely 1.6.4\n",
            "    Uninstalling Shapely-1.6.4:\n",
            "      Successfully uninstalled Shapely-1.6.4\n",
            "Successfully installed shapely-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### changed the version of opencv and shapely"
      ],
      "metadata": {
        "id": "AsEGfYX9X88M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sutdcv/Animal-Kingdom.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOPgiemKlu4x",
        "outputId": "06e08cfe-b674-488f-de83-4de6ea76e7a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Animal-Kingdom'...\n",
            "remote: Enumerating objects: 786, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 786 (delta 63), reused 114 (delta 49), pack-reused 646\u001b[K\n",
            "Receiving objects: 100% (786/786), 169.05 MiB | 32.82 MiB/s, done.\n",
            "Resolving deltas: 100% (224/224), done.\n",
            "Updating files: 100% (433/433), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the animal kingdom data to dataloader"
      ],
      "metadata": {
        "id": "FZufKsVG2ESq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CocoDetection\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "nG0zM_yivE_S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import tarfile\n",
        "\n",
        "# Change directory to the dataset folder\n",
        "%cd /content/drive/MyDrive/pose_estimation\n",
        "\n",
        "# Specify the path to the tar.gz file\n",
        "tar_file_path = '/content/drive/MyDrive/pose_estimation_dataset/dataset.tar.gz'\n",
        "\n",
        "# Open the tar.gz file\n",
        "with tarfile.open(tar_file_path, 'r') as tar:\n",
        "  # Extract the contents of the tar.gz file\n",
        "  tar.extractall(path='/content/dataset')\n",
        "\n",
        "# Print a success message\n",
        "print('Successfully extracted the tar.gz file')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSEHbS28yIsO",
        "outputId": "ea4368d3-1acd-4dc5-8d30-58fe2d416e2c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/pose_estimation'\n",
            "/content\n",
            "Successfully extracted the tar.gz file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading the data into a dataloader in preparation for training"
      ],
      "metadata": {
        "id": "iPgaMQQd3lNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H0DYd_-fcM5",
        "outputId": "8f9c1892-ddb5-44e7-f961-c489ac93eaf7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "class CustomAnimalDataset(Dataset):\n",
        "    def __init__(self, dataset_folder, annotations_file, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset_folder (string): Path to the folder containing images.\n",
        "            annotations_file (string): Path to the JSON file containing annotations.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.dataset_folder = dataset_folder\n",
        "        self.transform = transform\n",
        "        with open(annotations_file) as f:\n",
        "            self.annotations = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Extract information for the current image\n",
        "        annotation = self.annotations[idx]\n",
        "        img_path = os.path.join(self.dataset_folder, annotation['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Handle transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Extract other information from the annotation file\n",
        "        joints_vis = annotation.get('joints_vis', [])\n",
        "        scale = annotation.get('scale', 1.0)\n",
        "        center = annotation.get('center', [0, 0])\n",
        "\n",
        "        # Convert the lists in the annotation file to PyTorch tensors as required\n",
        "        scale = torch.tensor(annotation['scale'], dtype=torch.float32)\n",
        "        center = torch.tensor(annotation['center'], dtype=torch.float32)\n",
        "        joints = torch.tensor(annotation.get('joints', []), dtype=torch.float32)\n",
        "        joints_vis = torch.tensor(annotation.get('joints_vis', []), dtype=torch.float32)\n",
        "\n",
        "        return image, {'scale': scale, 'center': center, 'joints': joints, 'joints_vis': joints_vis}\n",
        "\n",
        "# Define your transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize images as needed\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "v285HSImjCKa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(dataset_folder, annotations_file, batch_size=4, shuffle=True, num_workers=0):\n",
        "    \"\"\"\n",
        "    Create a DataLoader for the custom animal dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_folder (string): Path to the folder containing images.\n",
        "        annotations_file (string): Path to the JSON file containing annotations.\n",
        "        batch_size (int): How many samples per batch to load.\n",
        "        shuffle (bool): Set to True to have the data reshuffled at every epoch.\n",
        "        num_workers (int): How many subprocesses to use for data loading.\n",
        "\n",
        "    Returns:\n",
        "        DataLoader: A DataLoader for the custom dataset.\n",
        "    \"\"\"\n",
        "    dataset = CustomAnimalDataset(\n",
        "        dataset_folder=dataset_folder,\n",
        "        annotations_file=annotations_file,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "# usage with the dataset of provided animal kingdom /content/dataset\n",
        "dataset_folder = '/content/dataset/dataset'\n",
        "annotations_file = '/content/drive/MyDrive/pose_estimation_dataset/annotation/ak_P3_bird/train.json'\n",
        "\n",
        "\n",
        "dataloader = create_dataloader(dataset_folder, annotations_file, batch_size=4, shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "s3jg2CyFksuM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_dataloader(dataloader):\n",
        "  for i, batch in enumerate(dataloader):\n",
        "    if i >= 1: # can change to see more than 1 batch\n",
        "      break\n",
        "    print(len(batch))\n",
        "    print(batch)\n",
        "\n",
        "print_dataloader(dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXW7HrS5k8vE",
        "outputId": "c064ee3c-2ede-4900-be81-a1f6af65b758"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "[tensor([[[[0.3647, 0.3490, 0.3255,  ..., 0.0510, 0.0667, 0.0745],\n",
            "          [0.3569, 0.3451, 0.3216,  ..., 0.0510, 0.0627, 0.0745],\n",
            "          [0.3490, 0.3373, 0.3176,  ..., 0.0549, 0.0667, 0.0784],\n",
            "          ...,\n",
            "          [0.5098, 0.5882, 0.5882,  ..., 0.1961, 0.1922, 0.1922],\n",
            "          [0.3412, 0.5686, 0.5882,  ..., 0.1961, 0.1922, 0.1922],\n",
            "          [0.2196, 0.5216, 0.5882,  ..., 0.1961, 0.1961, 0.1961]],\n",
            "\n",
            "         [[0.4627, 0.4471, 0.4235,  ..., 0.0824, 0.0902, 0.0863],\n",
            "          [0.4549, 0.4431, 0.4157,  ..., 0.0824, 0.0863, 0.0863],\n",
            "          [0.4471, 0.4353, 0.4157,  ..., 0.0824, 0.0902, 0.0902],\n",
            "          ...,\n",
            "          [0.5529, 0.6275, 0.6314,  ..., 0.2118, 0.2078, 0.2078],\n",
            "          [0.3765, 0.6039, 0.6314,  ..., 0.2118, 0.2078, 0.2078],\n",
            "          [0.2549, 0.5529, 0.6275,  ..., 0.2118, 0.2118, 0.2118]],\n",
            "\n",
            "         [[0.1451, 0.1294, 0.1137,  ..., 0.0000, 0.0039, 0.0000],\n",
            "          [0.1373, 0.1255, 0.1059,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.1294, 0.1176, 0.1020,  ..., 0.0000, 0.0039, 0.0039],\n",
            "          ...,\n",
            "          [0.4275, 0.5059, 0.5059,  ..., 0.1216, 0.1176, 0.1176],\n",
            "          [0.2784, 0.5020, 0.5176,  ..., 0.1216, 0.1176, 0.1176],\n",
            "          [0.1608, 0.4588, 0.5255,  ..., 0.1216, 0.1216, 0.1216]]],\n",
            "\n",
            "\n",
            "        [[[0.4588, 0.4667, 0.4784,  ..., 0.4667, 0.4627, 0.4588],\n",
            "          [0.4588, 0.4667, 0.4784,  ..., 0.4667, 0.4627, 0.4588],\n",
            "          [0.4627, 0.4706, 0.4784,  ..., 0.4667, 0.4627, 0.4588],\n",
            "          ...,\n",
            "          [0.6078, 0.5765, 0.5843,  ..., 0.4824, 0.4863, 0.4863],\n",
            "          [0.5843, 0.5412, 0.5686,  ..., 0.4824, 0.4863, 0.4863],\n",
            "          [0.5451, 0.5216, 0.5608,  ..., 0.4824, 0.4863, 0.4863]],\n",
            "\n",
            "         [[0.3725, 0.3804, 0.3922,  ..., 0.5686, 0.5647, 0.5608],\n",
            "          [0.3725, 0.3804, 0.3922,  ..., 0.5686, 0.5647, 0.5608],\n",
            "          [0.3765, 0.3843, 0.3922,  ..., 0.5686, 0.5647, 0.5608],\n",
            "          ...,\n",
            "          [0.5608, 0.5373, 0.5451,  ..., 0.5490, 0.5529, 0.5529],\n",
            "          [0.5451, 0.5059, 0.5373,  ..., 0.5490, 0.5529, 0.5529],\n",
            "          [0.5098, 0.4902, 0.5294,  ..., 0.5490, 0.5529, 0.5529]],\n",
            "\n",
            "         [[0.1373, 0.1451, 0.1608,  ..., 0.1373, 0.1333, 0.1294],\n",
            "          [0.1373, 0.1451, 0.1569,  ..., 0.1373, 0.1333, 0.1294],\n",
            "          [0.1412, 0.1490, 0.1608,  ..., 0.1373, 0.1333, 0.1294],\n",
            "          ...,\n",
            "          [0.3451, 0.2980, 0.2941,  ..., 0.1882, 0.1922, 0.1922],\n",
            "          [0.3255, 0.2627, 0.2706,  ..., 0.1882, 0.1922, 0.1922],\n",
            "          [0.2784, 0.2431, 0.2588,  ..., 0.1882, 0.1922, 0.1922]]],\n",
            "\n",
            "\n",
            "        [[[0.7098, 0.6902, 0.6549,  ..., 0.8784, 0.8784, 0.8784],\n",
            "          [0.7294, 0.7098, 0.6745,  ..., 0.8784, 0.8784, 0.8784],\n",
            "          [0.7451, 0.7255, 0.6902,  ..., 0.8784, 0.8784, 0.8784],\n",
            "          ...,\n",
            "          [0.4863, 0.4902, 0.4941,  ..., 0.3294, 0.3137, 0.3686],\n",
            "          [0.4863, 0.4902, 0.4941,  ..., 0.4353, 0.3725, 0.3882],\n",
            "          [0.4863, 0.4902, 0.4941,  ..., 0.5333, 0.4784, 0.4510]],\n",
            "\n",
            "         [[0.7373, 0.7176, 0.6824,  ..., 0.9176, 0.9176, 0.9176],\n",
            "          [0.7569, 0.7373, 0.7020,  ..., 0.9176, 0.9176, 0.9176],\n",
            "          [0.7765, 0.7529, 0.7176,  ..., 0.9176, 0.9176, 0.9176],\n",
            "          ...,\n",
            "          [0.5216, 0.5176, 0.5137,  ..., 0.3176, 0.3059, 0.3647],\n",
            "          [0.5216, 0.5176, 0.5137,  ..., 0.4196, 0.3647, 0.3843],\n",
            "          [0.5216, 0.5176, 0.5137,  ..., 0.5176, 0.4706, 0.4471]],\n",
            "\n",
            "         [[0.7059, 0.6863, 0.6510,  ..., 0.9255, 0.9255, 0.9255],\n",
            "          [0.7294, 0.7059, 0.6706,  ..., 0.9255, 0.9255, 0.9255],\n",
            "          [0.7490, 0.7255, 0.6902,  ..., 0.9255, 0.9255, 0.9255],\n",
            "          ...,\n",
            "          [0.4902, 0.4902, 0.4902,  ..., 0.3020, 0.2941, 0.3569],\n",
            "          [0.4902, 0.4902, 0.4902,  ..., 0.4078, 0.3569, 0.3804],\n",
            "          [0.4902, 0.4902, 0.4902,  ..., 0.5098, 0.4627, 0.4471]]],\n",
            "\n",
            "\n",
            "        [[[0.8039, 0.8078, 0.8196,  ..., 0.8353, 0.8157, 0.7255],\n",
            "          [0.8039, 0.8078, 0.8157,  ..., 0.7961, 0.8235, 0.8000],\n",
            "          [0.8039, 0.8078, 0.8157,  ..., 0.7098, 0.8275, 0.8549],\n",
            "          ...,\n",
            "          [0.7686, 0.6784, 0.7843,  ..., 0.4392, 0.4627, 0.4824],\n",
            "          [0.7725, 0.6745, 0.7804,  ..., 0.4706, 0.4588, 0.4745],\n",
            "          [0.7725, 0.6667, 0.7961,  ..., 0.4824, 0.4549, 0.4510]],\n",
            "\n",
            "         [[0.8039, 0.8078, 0.8157,  ..., 0.8314, 0.8118, 0.7216],\n",
            "          [0.8039, 0.8078, 0.8118,  ..., 0.7922, 0.8196, 0.7961],\n",
            "          [0.8039, 0.8078, 0.8118,  ..., 0.7059, 0.8235, 0.8510],\n",
            "          ...,\n",
            "          [0.7725, 0.6784, 0.7843,  ..., 0.4314, 0.4549, 0.4745],\n",
            "          [0.7765, 0.6745, 0.7804,  ..., 0.4627, 0.4510, 0.4667],\n",
            "          [0.7765, 0.6667, 0.7922,  ..., 0.4745, 0.4471, 0.4431]],\n",
            "\n",
            "         [[0.8431, 0.8471, 0.8549,  ..., 0.8510, 0.8314, 0.7412],\n",
            "          [0.8431, 0.8471, 0.8549,  ..., 0.8118, 0.8392, 0.8157],\n",
            "          [0.8431, 0.8471, 0.8549,  ..., 0.7255, 0.8431, 0.8706],\n",
            "          ...,\n",
            "          [0.7961, 0.7098, 0.8196,  ..., 0.4431, 0.4667, 0.4863],\n",
            "          [0.8000, 0.7059, 0.8157,  ..., 0.4745, 0.4627, 0.4784],\n",
            "          [0.8000, 0.6980, 0.8314,  ..., 0.4863, 0.4588, 0.4549]]]]), {'scale': tensor([1.3316, 1.1935, 0.8953, 1.3079]), 'center': tensor([[314.1255, 226.8400],\n",
            "        [347.6624, 203.8005],\n",
            "        [320.7419, 205.5906],\n",
            "        [159.5761, 231.7678]]), 'joints': tensor([[[307.2548,  93.6800],\n",
            "         [291.3232, 103.5286],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [276.2510, 110.7702],\n",
            "         [288.4266, 111.3495],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [276.4062, 111.0599],\n",
            "         [301.0000, 118.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [284.0000, 134.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [352.0000, 154.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [340.0000, 221.0000],\n",
            "         [344.0000, 251.0000],\n",
            "         [346.0000, 360.0000]],\n",
            "\n",
            "        [[353.6801,  84.4463],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [369.8718, 121.1763],\n",
            "         [449.6410, 143.8723],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [392.7521, 136.7781],\n",
            "         [450.2650, 153.2857],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [287.7661, 152.9950],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [245.0598, 178.2585],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [334.9801, 227.3214],\n",
            "         [361.8710, 276.7601],\n",
            "         [278.5657, 280.9538],\n",
            "         [358.5608, 292.4509],\n",
            "         [270.7627, 292.1889],\n",
            "         [355.6215, 312.4899],\n",
            "         [265.0948, 323.1547],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000]],\n",
            "\n",
            "        [[245.7582, 155.0326],\n",
            "         [239.7034, 170.0538],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [231.2083, 185.1136],\n",
            "         [240.3039, 176.1340],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [231.6748, 186.4796],\n",
            "         [274.2381, 168.3888],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [288.8135, 192.1995],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [370.8919, 221.4073],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [327.8857, 173.9150],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [369.1149, 224.5822],\n",
            "         [383.2482, 238.9655],\n",
            "         [410.2754, 256.1487]],\n",
            "\n",
            "        [[290.3704, 200.1316],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [246.0082, 241.6342],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [213.3896, 263.4040],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [115.6898, 228.9829],\n",
            "         [193.5247, 206.0961],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [ -1.0000,  -1.0000],\n",
            "         [175.1572, 214.1618],\n",
            "         [148.1304, 223.0145],\n",
            "         [ 28.7819, 239.2658]]]), 'joints_vis': tensor([[1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 1., 1.],\n",
            "        [1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 0., 0., 0.],\n",
            "        [1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 1., 1.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 1., 1.]])}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a training data loader that has the annotations we care about and the image, now we can try to come up with a baseline model.\n",
        "\n",
        "this includes:  defining the model architecture, setting up the loss function, and running the training loop.\n",
        "\n",
        "this phd student has implemented deeplabv3 -> https://github.com/chenxi116/DeepLabv3.pytorch\n",
        "there are a bunch of backbone models we could use from here -> https://github.com/open-mmlab/mmpose/blob/main/mmpose/models/backbones/__init__.py\n"
      ],
      "metadata": {
        "id": "drTe11Fqu4PC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XBKsgduBuu_-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}